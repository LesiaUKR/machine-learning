# warnings: для обробки попереджень під час виконання коду.
# pandas as pd: для роботи з табличними даними (DataFrame), маніпуляції 
# та аналізу даних.
# numpy as np: для числових операцій і роботи з багатовимірними масивами.
# category_encoders as ce: для кодування категоріальних змінних, зокрема 
# використання TargetEncoder.
# train_test_split з sklearn.model_selection: для розділення даних на 
# тренувальний та тестовий набори.
# KBinsDiscretizer, StandardScaler з sklearn.preprocessing: для дискретизації 
# числових змінних та стандартизації ознак.
# SVC з sklearn.svm: для створення класифікатора на основі методу опорних 
# векторів (SVM).
# accuracy_score, confusion_matrix з sklearn.metrics: для оцінки точності 
# моделі та аналізу результатів класифікації.
# %%
import warnings
import pandas as pd
import numpy as np
import category_encoders as ce
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import KBinsDiscretizer, StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix

# %%
# Завантажуємо датасет з файлу petfinder_data.csv.gz, розташованого за вказаним 
# шляхом. Використовуємо data.info() для отримання загальної інформації 
# про датасет: кількість записів, стовпців, типи даних та кількість ненульових 
# значень у кожному стовпці. Це допомагає зрозуміти структуру та зміст даних

data = pd.read_csv('./datasets/mod_04_topic_08_petfinder_data.csv.gz')
data.info()

# %%
# Обчислюємо кількість унікальних значень у кожному стовпці датасету за 
# допомогою nunique(). Це корисно для визначення того, які змінні є 
# категоріальними, а які — числовими, а також для виявлення можливих проблем 
# з даними, наприклад, якщо стовпець має лише одне унікальне значення
data.nunique()

# %%
# Відображаємо перші п'ять записів стовпця 'Description' за допомогою head(). 
# Це дозволяє переглянути зразки описів тварин, які можуть містити текстові 
# дані. Перевіряємо вміст цього стовпця перед подальшою обробкою або видаленням
data['Description'].head()

# %%
# Видаляємо стовпець 'Description' з датасету, оскільки текстові описи можуть 
# бути складними для обробки і, ймовірно, не будуть використовуватися в моделі. 
# Параметр inplace=True вказує на те, що зміни повинні бути внесені 
# безпосередньо в оригінальний DataFrame data.

data.drop('Description', axis=1, inplace=True)

# %%
# Використовуємо value_counts() для підрахунку кількості записів у кожній 
# категорії змінної 'AdoptionSpeed', яка, ймовірно, представляє швидкість 
# усиновлення тварини. Метод sort_index() сортує результати за значеннями 
# індексу (тобто за значеннями 'AdoptionSpeed'). 
# Це допомагає зрозуміти розподіл цільової змінної
data['AdoptionSpeed'].value_counts().sort_index()

# %%
# Використовуємо np.where() для перетворення змінної 'AdoptionSpeed' 
# у бінарну змінну:

# Якщо значення 'AdoptionSpeed' дорівнює 4 (ймовірно, тварина не була 
# усиновлена або була усиновлена дуже пізно), присвоюємо значення 0.
# В іншому випадку (значення від 0 до 3, тобто тварина була усиновлена швидше), 
# присвоюємо значення 1.
# Після цього знову використовуємо value_counts() для перевірки розподілу 
# нових бінарних класів.

# Це перетворення дозволяє спростити задачу до бінарної класифікації: 
# усиновлена тварина швидко (1) або ні (0)

data['AdoptionSpeed'] = np.where(data['AdoptionSpeed'] == 4, 0, 1)
data['AdoptionSpeed'].value_counts()

# %%
# Перетворюємо стовпець 'Fee' (ймовірно, плата за усиновлення) наступним чином:

# astype(bool): перетворюємо числові значення в булеві. Будь-яке ненульове 
# значення стає True, нульові значення — False.
# astype(int): перетворюємо булеві значення в цілі числа (1 для True, 0 
#                                                         для False).
# astype(str): перетворюємо цілі числа в рядки.
# У результаті отримуємо стовпець, де значення '0' або '1' представлені як рядки.
# Це підготовка до подальшого кодування, оскільки ми хочемо, щоб всі ознаки 
# були категоріальними (тип str)

data['Fee'] = data['Fee'].astype(bool).astype(int).astype(str)

# %%
# Розділяємо дані на вхідні ознаки X та цільову змінну y:

# X — всі стовпці, окрім 'AdoptionSpeed'.
# y — стовпець 'AdoptionSpeed'.
# Використовуємо train_test_split для поділу даних на тренувальний та тестовий набори:

# test_size=0.2: 20% даних відводиться під тестовий набір.
# random_state=42: фіксуємо генератор випадкових чисел для відтворюваності результатів.
# У результаті отримуємо чотири змінні: X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = (
    train_test_split(
        data.drop('AdoptionSpeed', axis=1),
        data['AdoptionSpeed'],
        test_size=0.2,
        random_state=42))

# %%
# Визначаємо числові стовпці num_cols у тренувальному наборі за допомогою 
# select_dtypes(exclude='object'), щоб отримати назви стовпців з числовими даними.

# Ініціалізуємо KBinsDiscretizer з параметром encode='ordinal', який буде 
# використовуватися для дискретизації числових ознак шляхом розбиття їх на інтервали (бінінг).

# Навчаємо дискретизатор kbins на числових ознаках тренувального набору X_train[num_cols].

# Трансформуємо числові ознаки в тренувальному та тестовому наборах:

# Використовуємо kbins.transform() для перетворення числових значень на категорії (номери бінів).
# Перетворюємо результати в цілі числа за допомогою astype(int).
# Потім перетворюємо в рядки за допомогою astype(str), щоб зробити всі ознаки категоріальними.
# Мета цього кроку — перетворити числові змінні на категоріальні, 
# щоб потім застосувати до них категоріальне кодування

num_cols = X_train.select_dtypes(exclude='object').columns

kbins = KBinsDiscretizer(encode='ordinal').fit(X_train[num_cols])

X_train[num_cols] = (kbins
                     .transform(
                         X_train[num_cols])
                     .astype(int)
                     .astype(str))

X_test[num_cols] = (kbins
                    .transform(
                        X_test[num_cols])
                    .astype(int)
                    .astype(str))

# %%
# Ініціалізуємо TargetEncoder з бібліотеки category_encoders. Цей енкодер 
# замінює категоріальні значення на середнє значення цільової змінної для кожної категорії.

# Навчаємо енкодер на тренувальному наборі X_train та цільовій змінній 
# y_train за допомогою fit_transform().

# Трансформуємо тестовий набір X_test за допомогою навченого енкодера.

# Відображаємо перші п'ять записів перетвореного тренувального набору 
# X_train.head() для перевірки.

# Це кодування допомагає зберегти інформацію про зв'язок між 
# категоріальними ознаками та цільовою змінною.

encoder = ce.TargetEncoder()

X_train = encoder.fit_transform(X_train, y_train)
X_test = encoder.transform(X_test)

X_train.head()

# %%

# Ініціалізуємо StandardScaler для стандартизації ознак: кожна ознака буде 
# мати середнє значення 0 та стандартне відхилення 1.

# Метод set_output(transform='pandas') забезпечує, що вихід після трансформації 
# буде у форматі DataFrame, а не numpy масиву.

# Навчаємо скейлер на тренувальному наборі X_train та трансформуємо його за 
# допомогою fit_transform().

# Трансформуємо тестовий набір X_test за допомогою навченого скейлера.

# Стандартизація ознак важлива для алгоритмів, які чутливі до масштабу 
# даних, таких як SVM.

scaler = StandardScaler().set_output(transform='pandas')

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# %%
# Створюємо класифікатор на основі методу опорних векторів (SVC):

# class_weight='balanced': автоматично налаштовує ваги класів, компенсуючи дисбаланс у даних.
# kernel='poly': використовує поліноміальне ядро для виявлення нелінійних зв'язків.
# probability=True: дозволяє обчислювати ймовірності приналежності до класів.
# random_state=42: для відтворюваності результатів.
# Навчаємо модель на тренувальних даних X_train та y_train

clf = SVC(class_weight='balanced',
          kernel='poly',
          probability=True,
          random_state=42)

clf.fit(X_train, y_train)

# %%
# Використовуємо навчену модель clf для прогнозування класів на тестовому наборі X_test.

# Обчислюємо матрицю невірної класифікації за допомогою confusion_matrix(y_test,
# preds). Матриця показує, скільки об'єктів було правильно та неправильно 
# класифіковано для кожного класу. Це допомагає оцінити ефективність моделі 
# та зрозуміти, де вона робить помилки.

preds = clf.predict(X_test)

confusion_matrix(y_test, preds)

# %%
# Обчислюємо точність моделі за допомогою accuracy_score(y_test, preds), 
# яка показує частку правильно передбачених класів від загальної кількості.

# Виводимо результат у відсотках з одним знаком після коми, наприклад: 
# "Model accuracy is: 75.3%"

print(f'Model accuracy is: {accuracy_score(y_test, preds):.1%}')

# %%
# Створюємо новий DataFrame pet, який містить інформацію про конкретну тварину,
# для якої ми хочемо спрогнозувати ймовірність усиновлення.

# Вказуємо значення для всіх необхідних ознак:

# 'Type': 'Cat': тип тварини — кіт.
# 'Age': 3': вік тварини.
# Інші ознаки описують породу, стать, колір, розмір, стан здоров'я тощо.
# Використовуємо index=[0] для створення DataFrame з одним записом.

pet = pd.DataFrame(
    data={
        'Type': 'Cat',
        'Age': 3,
        'Breed1': 'Tabby',
        'Gender': 'Male',
        'Color1': 'Black',
        'Color2': 'White',
        'MaturitySize': 'Small',
        'FurLength': 'Short',
        'Vaccinated': 'No',
        'Sterilized': 'No',
        'Health': 'Healthy',
        'Fee': True,
        'PhotoAmt': 2,
    },
    index=[0])

# %%
# Дискретизуємо числові ознаки тварини pet так само, як це робили з тренувальними даними:

# Використовуємо kbins.transform(pet[num_cols]) для перетворення числових значень.
# Перетворюємо результати в цілі числа та рядки, щоб ознаки відповідали формату
# даних, на яких навчалась модель.
# Використовуємо блок with warnings.catch_warnings() та warnings.simplefilter('ignore'),
#  щоб ігнорувати можливі попередження під час трансформацій.

# Прогнозуємо ймовірність усиновлення тварини:

# encoder.transform(pet): кодуємо ознаки тварини за допомогою навченого TargetEncoder.
# scaler.transform(...): масштабуємо ознаки тварини за допомогою навченого StandardScaler.
# clf.predict_proba(...): отримуємо ймовірності приналежності до кожного класу.
# flatten(): перетворюємо двовимірний масив ймовірностей в одномірний.
# Виводимо ймовірність того, що ця тварина буде усиновлена (ймовірність класу 1) 
# у відсотках з одним знаком після коми, наприклад: 
# "This pet has a 85.7% probability of getting adopted"

pet[num_cols] = kbins.transform(pet[num_cols]).astype(int).astype(str)

with warnings.catch_warnings():
    warnings.simplefilter('ignore')
    prob = (clf
            .predict_proba(
                scaler
                .transform(
                    encoder
                    .transform(
                        pet)))
            .flatten())

print(f'This pet has a {prob[1]:.1%} probability "of getting adopted"')
